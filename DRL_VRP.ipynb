{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1cXvdhvMxCM",
        "outputId": "a53f5d3f-bd2e-426d-bcfb-0874e9321501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 19 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.48.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed gast-0.4.0 keras-2.10.0 tensorboard-2.10.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.14.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.1.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Collecting tensorflow-probability==0.17.0\n",
            "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 74.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Collecting gym<=0.23.0,>=0.17.0\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[K     |████████████████████████████████| 624 kB 83.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 59.0 MB/s \n",
            "\u001b[?25hCollecting rlds\n",
            "  Downloading rlds-0.1.5-py3-none-manylinux2010_x86_64.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting dm-reverb~=0.9.0\n",
            "  Downloading dm_reverb-0.9.0-cp37-cp37m-manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 82.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.10.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.17.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.17.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.17.0->tf-agents[reverb]) (0.1.7)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.9.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (3.8.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (0.26.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (2.10.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (1.48.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (21.3)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (2.10.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (2.10.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-agents[reverb]) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.10.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.10.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.10.0->tf-agents[reverb]) (3.0.9)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697659 sha256=a3f3fcae116202fb09777c9fdd5bd6b94f56c78bb03df09e07546f6a7713dee7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/7e/16/4d727df048fdb96518ec5c02266e55b98bc398837353852a6a\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf-agents, rlds, dm-reverb\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.16.0\n",
            "    Uninstalling tensorflow-probability-0.16.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.16.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed dm-reverb-0.9.0 gym-0.23.0 pygame-2.1.0 rlds-0.1.5 tensorflow-probability-0.17.0 tf-agents-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVEXUi9BOp2n"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbcs9MutM6Sv"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.specs import from_spec\n",
        "from tf_agents.networks import sequential\n",
        "#from tf_agents.networks import mask_splitter_network\n",
        "from tf_agents.networks.mask_splitter_network import MaskSplitterNetwork\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import replay_buffer\n",
        "from tf_agents.utils import lazy_loader\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "\n",
        "import reverb\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6V_t216Ot_o"
      },
      "source": [
        "# **VRP parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_NAYgDNNCZ8"
      },
      "outputs": [],
      "source": [
        "#VRP features\n",
        "N = 20  # number of customers try : 0, 20, 50 and 100\n",
        "customers = np.array(range(2, N+1)) #list of customers\n",
        "#customers = list(customers)\n",
        "M = 4 # number of vehicles\n",
        "max_capacity = 25 # Vehicles capacity coreesponding to number of customers 20, 30, 40 and 50\n",
        "ENV_SIZE = N\n",
        "Loads = np.zeros(shape=N)\n",
        "Distance = np.zeros(shape=(N,N)) # distance matrix\n",
        "coords = np.random.rand(N,2)*ENV_SIZE #or randomly generated in the unit square [0,1] * [0,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP0iRA7wOyDB"
      },
      "source": [
        "# **DQN parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnBWPlULNDhe"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (100,)\n",
        "\n",
        "#num_iterations = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = N+M+1  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = N+M+1 # N+M+1 # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100#100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 30  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-2  # @param {type:\"number\"}\n",
        "log_interval = N+M+1 #200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 100  # @param {type:\"integer\"}\n",
        "epsilon_greedyp = 0.6\n",
        "\n",
        "num_iterations = batch_size * eval_interval\n",
        "\n",
        "# number of examples = n_epochs * n_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chs2ud38Omgp"
      },
      "source": [
        "# **Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY61qH_7NGay"
      },
      "outputs": [],
      "source": [
        "class VRPenvironement2(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self,N,M,customers,coords, Loads,max_capacity):\n",
        "\n",
        "        # Store default parameters\n",
        "        self.N = N\n",
        "        self.customers = customers\n",
        "        self.M = M\n",
        "        self.coords = coords\n",
        "        self.Loads = Loads\n",
        "        self.max_capacity = max_capacity\n",
        "\n",
        "        self.Loads = np.random.randint(1,high=9, size=self.N) # chosen uniformly at random\n",
        "        print(\"ld \",self.Loads)\n",
        "        self.Loads[0]=0\n",
        "        print(\"ld \",self.Loads)\n",
        "        self.Distance = cdist(self.coords, self.coords)\n",
        "        print(self.Distance)\n",
        "\n",
        "        self.Depot = 1\n",
        "        self.Loaded_weight = 0\n",
        "        self.Tot_Cost = 0\n",
        "\n",
        "        self.lg = 0\n",
        "        self.nv = 1\n",
        "\n",
        "        # Configuration for RL agent\n",
        "        self._observation_spec = {\n",
        "                                'observations': array_spec.ArraySpec(shape=(self.N + self.M + 1,), dtype=np.int32, name='observations'),\n",
        "                                'valid_actions': array_spec.ArraySpec(shape=(self.N - 1,), dtype=bool, name='valid_actions')\n",
        "                                }\n",
        "        self._reward = array_spec.ArraySpec(shape=(), dtype=np.float32, name='reward')\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=self.N - 2, name='action')\n",
        "        self._step_type = array_spec.ArraySpec(shape=(), dtype=np.int32, name='step_type')\n",
        "\n",
        "        self._state = np.array([], dtype=np.int32)\n",
        "        self._state = np.zeros(self.N + self.M + 1)\n",
        "        self._masks = np.full((self.N - 1,), True, dtype=bool)\n",
        "\n",
        "\n",
        "        self.ls = 0\n",
        "        self.addedC = 0\n",
        "        self.X = np.zeros((self.N,self.N))\n",
        "        self.reset()\n",
        "        print(\"after 1st reset\")\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def observation_spec1(self):\n",
        "    return self._observation_spec['observations']\n",
        "\n",
        "  def __is_cust_in(self, index):\n",
        "    for item in self._state:\n",
        "        if item == index:\n",
        "          return 1\n",
        "    return 0\n",
        "\n",
        "  def __all_road_occupied(self):\n",
        "    sc = 0\n",
        "    for i in range(0,N):\n",
        "      for item in self._state:\n",
        "        if item == i:\n",
        "          sc = sc + 1\n",
        "    return (sc == N)\n",
        "\n",
        "  def __compute_total_Cost(self):\n",
        "\n",
        "        self.X[int(self._state[self.ls-1]-1),0] = 1\n",
        "        Tcost = -1 * np.sum(self.Distance @ self.X)\n",
        "        self.X[int(self._state[self.ls-1]-1),0] = 0\n",
        "        return Tcost\n",
        "\n",
        "  def __compute_reward(self):\n",
        "        self.Tot_Cost = -1 * self.__compute_total_Cost()\n",
        "\n",
        "        return self.Tot_Cost\n",
        "\n",
        "  #def __observation_action_splitter(self):\n",
        "\t#  return self._observation_spec['observations'], self._observation_spec['valid_actions']\n",
        "\n",
        "  def __obs(self, obs, mask):\n",
        "    return {'observations': obs, 'valid_actions': mask}\n",
        "\n",
        "  def __observation_and_action_constraint_splitter(self):\n",
        "     #self._mask = np.full((self.N - 2,), False, dtype=bool)\n",
        "     for elem in self._state:\n",
        "          if elem != 1 and elem != 0:\n",
        "               self._masks[int(elem) - 2] = False #valid action\n",
        "\n",
        "     return tf.convert_to_tensor(self._state, dtype=np.int32), tf.convert_to_tensor(self._masks, dtype=bool)\n",
        "\n",
        "  #def _splitter_fn(self):\n",
        "  #  self.observation_spec['observations'], self._observation_spec['valid_actions'] = self.__observation_and_action_constraint_splitter()\n",
        "  #  return self.observation_spec['observations'], self.observation_spec['valid_actions']\n",
        "\n",
        "  def _reset(self):\n",
        "    print(\"reset\")\n",
        "    self.lg = 0\n",
        "    self._reward = 0\n",
        "    self.Loaded_weight = 0\n",
        "    self._discount = 1.0\n",
        "    self.ls = 0\n",
        "    self.addedC = 1\n",
        "    self.Tot_Cost = 0\n",
        "\n",
        "    state = np.array([], dtype=np.int32)\n",
        "    state = np.zeros(self.N + self.M + 1)\n",
        "\n",
        "    self._state = state\n",
        "    self._masks = np.full((self.N - 1,), True, dtype=bool)\n",
        "    #obs['observation'] = get_the_env_observations(…)\n",
        "    #obs['valid_actions'] = get_valid_actions_masks(…) # True for valid and False for invalid actions\n",
        "    print(self._state)\n",
        "    self._episode_ended = False\n",
        "\n",
        "    #return ts.restart([self.__obs(np.array(self._state, dtype=np.int32), self._masks)])\n",
        "    return ts.restart(self.__obs(np.array(self._state, dtype=np.int32), self._masks))\n",
        "    #return ts.restart([self.__obs(self._state, self._masks)])\n",
        "\n",
        "  def _step(self, action):\n",
        "    print(\"step: \", action+2)\n",
        "    #masks = get_valid_actions_masks(…)\n",
        "    if not self._masks[action]:\n",
        "      print(\"Error: Invalid action is taken\")\n",
        "    #  #sys.exit(-1)\n",
        "    else:\n",
        "      self._masks[action] = False\n",
        "\n",
        "    if self.addedC == self.N:\n",
        "      return ts.termination(self.__obs(np.array(self._state, dtype=np.int32), self._masks), self._reward)\n",
        "    print(\"action: \", action+2)\n",
        "    self.lg = self.lg + 1\n",
        "    if(self.lg > 3*N):\n",
        "      #return ts.termination([self.__obs(np.array(self._state, dtype=np.int32), self._masks)], self._reward)\n",
        "      return ts.termination(self.__obs(np.array(self._state, dtype=np.int32), self._masks), self._reward)\n",
        "\n",
        "    if self.ls == 0:\n",
        "\n",
        "        self._state[self.ls] = 1\n",
        "        self.ls = self.ls + 1\n",
        "        self.addedC = 1\n",
        "        self.Loaded_weight = 0\n",
        "        #ts.transition([self.__obs(np.array(self._state, dtype=np.int32), self._masks)], self._reward)\n",
        "        ts.transition(self.__obs(np.array(self._state, dtype=np.int32), self._masks), self._reward)\n",
        "\n",
        "    if self.__is_cust_in(action+2) == 0:\n",
        "        print(\"max_capacity \",self.max_capacity)\n",
        "\n",
        "        print(\"Loaded_weight \", self.Loaded_weight)\n",
        "        if((self.Loaded_weight + self.Loads[action+1])  < self.max_capacity):\n",
        "            print(\"1Loaded_weight+ \", (self.Loaded_weight + self.Loads[action+1]))\n",
        "\n",
        "            self.X[int(self._state[self.ls-1]-1),int(action+1)] = 1\n",
        "            self._state[self.ls] = int(action+1 + 1)\n",
        "            self.ls = self.ls + 1\n",
        "            self.addedC = self.addedC + 1\n",
        "            self.Loaded_weight = self.Loaded_weight + self.Loads[action+1]\n",
        "\n",
        "        else:\n",
        "            print(\"add an other car\")\n",
        "            #increment number of vehicles\n",
        "            self.nv = self.nv + 1\n",
        "\n",
        "            #if(self._state[len(self._state)-1] != 1):\n",
        "            if(self._state[self.ls-1] != 1):\n",
        "              self._state[self.ls] = 1\n",
        "              self.ls = self.ls + 1\n",
        "              self.X[int(self._state[self.ls-1]-1),0] = 1\n",
        "              self.Loaded_weight = 0\n",
        "            # Action of adding customer c at the end\n",
        "            if((self.Loaded_weight + self.Loads[action+1])  < self.max_capacity):\n",
        "              print(\"Loaded_weight+ \", (self.Loaded_weight + self.Loads[action+1]))\n",
        "              # Action of adding customer c at the end\n",
        "              self.X[int(self._state[self.ls-1]-1),action+1] = 1\n",
        "              self._state[self.ls] = int(action+1+1)\n",
        "              self.ls = self.ls + 1\n",
        "              self.addedC = self.addedC + 1\n",
        "              self.Loaded_weight = self.Loaded_weight + self.Loads[action+1]\n",
        "\n",
        "\n",
        "        if self.addedC == self.N:\n",
        "          self._state[self.ls] = 1\n",
        "          self.ls = self.ls + 1\n",
        "          self.X[int(self._state[self.ls-1]-1),0] = 1\n",
        "        #Computing and returning the reward\n",
        "        if(self.ls >= 2):\n",
        "          self.Tot_Cost = self.Tot_Cost + (ENV_SIZE - self.Distance[int(self._state[int(self.ls-2)]-1),int(self._state[int(self.ls-1)]-1)] * self.X[int(self._state[int(self.ls-2)]-1),int(self._state[int(self.ls-1)]-1)])\n",
        "        else:\n",
        "          self.Tot_Cost = 0\n",
        "\n",
        "        #self._reward = self._reward + (1000 - self.Tot_Cost)\n",
        "        self._reward = self._reward + self.Tot_Cost\n",
        "        print(\"gr: \",self._reward)\n",
        "        s_next = action\n",
        "        self._episode_ended = (self.addedC == self.N)\n",
        "        print(\"ending episode1\", self._episode_ended)\n",
        "\n",
        "        if self._episode_ended:\n",
        "          #return ts.termination([self.__obs(np.array(self._state, dtype=np.int32), self._masks)], self._reward)\n",
        "          return ts.termination(self.__obs(np.array(self._state, dtype=np.int32), self._masks), self._reward)\n",
        "        else:\n",
        "          #return ts.transition([self.__obs(np.array(self._state, dtype=np.int32), self._masks)], self._reward)\n",
        "          return ts.transition(self.__obs(np.array(self._state, dtype=np.int32), self._masks), self._reward)\n",
        "\n",
        "    else:\n",
        "\n",
        "        #self._reward = -1 * (self._reward + 2 * ENV_SIZE) #-math.inf\n",
        "        self._reward = self._reward - 2 * ENV_SIZE\n",
        "        print(\"br: \",self._reward)\n",
        "        #return ts.transition([self.__obs(np.array(self._state, dtype=np.int32), self._masks)], self._reward)\n",
        "        return ts.transition(self.__obs(np.array(self._state, dtype=np.int32), self._masks), self._reward)\n",
        "\n",
        "        self._episode_ended = (self.addedC == self.N)\n",
        "        print(\"ending episode2\", self._episode_ended)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frbgY_IaWOoi"
      },
      "outputs": [],
      "source": [
        "#def splitter_fn(observation_and_mask):\n",
        "#    return observation_and_mask['observations'], observation_and_mask['valid_actions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ottJqYI6_ub4"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(observation):\n",
        "     print(\"observation \", observation)\n",
        "     print(\"state \", observation['observations'])\n",
        "\n",
        "     #state = (observation[0][0])\n",
        "     #mask = np.full((N - 2,), False, dtype=bool)\n",
        "     #for elem in state:\n",
        "     #     if elem != 1 and elem != 0:\n",
        "     #          mask[int(elem) - 2] = False #valid action\n",
        "\n",
        "     #return observation['observations'], tf.convert_to_tensor(mask, dtype=bool)\n",
        "\n",
        "def splitter_fn(observation_and_mask):\n",
        "    #observation_and_mask['observations'], observation_and_mask['valid_actions'] = observation_and_action_constraint_splitter(observation_and_mask)\n",
        "    #print(env.observation_spec['observations'])\n",
        "    #print(env.observation_spec['valid_actions'])\n",
        "    return observation_and_mask['observations'], observation_and_mask['valid_actions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpsIO468O-YO",
        "outputId": "4ac27e48-21e2-47f7-e63f-2b4ce7effbac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ld  [5 4 4 8 4 8 3 3 5 2 2 1 4 3 4 8 6 6 2 6]\n",
            "ld  [0 4 4 8 4 8 3 3 5 2 2 1 4 3 4 8 6 6 2 6]\n",
            "[[ 0.          3.95198098  6.01052114 12.31455169  7.30384382 11.1883957\n",
            "  15.36308519 13.91485782 16.59536929  2.76940835 14.01206717  8.33708092\n",
            "  16.59867848 14.27413627 11.04137357  6.62991984 14.33070509 14.8669568\n",
            "   8.88269831  5.93568556]\n",
            " [ 3.95198098  0.          5.86654744 14.83201044  3.36218314  7.35889893\n",
            "  13.54012009 11.82303699 16.20482995  2.03323992 13.77558707  4.59614396\n",
            "  16.47124698 13.01868887 13.60851248  5.86544785 15.4020818  11.77159113\n",
            "  10.91724302  2.68631492]\n",
            " [ 6.01052114  5.86654744  0.         10.05775251  7.31312299  9.39084217\n",
            "   9.59623065  8.39404245 10.69708633  6.94087633  8.15474887  6.9216408\n",
            "  10.8161436   8.3006773   8.99732144  1.01600215  9.57902657 10.53976965\n",
            "   5.96881341  8.4594488 ]\n",
            " [12.31455169 14.83201044 10.05775251  0.         17.17292312 19.40959616\n",
            "  15.49187863 15.46418223 11.8163843  14.81219171 10.06118069 16.97286589\n",
            "  10.99741633 13.21094063  1.27469301 10.84891727  5.99882853 19.20073763\n",
            "   4.0916658  17.48364466]\n",
            " [ 7.30384382  3.36218314  7.31312299 17.17292312  0.          4.12121152\n",
            "  12.41737791 10.53740445 16.26913679  5.1593172  14.10325725  1.81443808\n",
            "  16.74853586 12.48424171 16.01936434  6.83215712 16.68950867  9.35281869\n",
            "  13.10836514  3.09728046]\n",
            " [11.1883957   7.35889893  9.39084217 19.40959616  4.12121152  0.\n",
            "  10.71765502  8.82279823 15.78628045  9.25859306 14.08948968  2.87610215\n",
            "  16.49947996 11.54335036 18.38186105  8.56067924 17.66629151  6.12402049\n",
            "  15.33858493  7.01000059]\n",
            " [15.36308519 13.54012009  9.59623065 15.49187863 12.41737791 10.71765502\n",
            "   0.          1.91844727  6.14012873 15.35789091  5.92437391 10.73046102\n",
            "   7.16585263  2.33328847 15.07301839  8.78716436 10.79849987  5.87769296\n",
            "  12.55375999 15.20649453]\n",
            " [13.91485782 11.82303699  8.39404245 15.46418223 10.53740445  8.82279823\n",
            "   1.91844727  0.          7.49908512 13.69780528  6.6634543   8.83071611\n",
            "   8.42572334  3.19223347 14.89749402  7.49228063 11.36067587  4.45728623\n",
            "  12.16228168 13.37487057]\n",
            " [16.59536929 16.20482995 10.69708633 11.8163843  16.26913679 15.78628045\n",
            "   6.14012873  7.49908512  0.         17.58073609  2.59588561 14.91113578\n",
            "   1.10703241  4.34892828 11.87430471 10.3940824   6.00052757 11.90934035\n",
            "  10.35796599 18.48213623]\n",
            " [ 2.76940835  2.03323992  6.94087633 14.81219171  5.1593172   9.25859306\n",
            "  15.35789091 13.69780528 17.58073609  0.         15.07344853  6.57685327\n",
            "  17.7487424  14.66836457 13.55014187  7.1960052  16.15616614 13.80271508\n",
            "  11.13529619  3.20918391]\n",
            " [14.01206717 13.77558707  8.15474887 10.06118069 14.10325725 14.08948968\n",
            "   5.92437391  6.6634543   2.59588561 15.07344853  0.         12.87923831\n",
            "   2.6965209   3.6237403   9.90750697  7.92191087  4.88286527 11.08398952\n",
            "   8.02657554 16.1384518 ]\n",
            " [ 8.33708092  4.59614396  6.9216408  16.97286589  1.81443808  2.87610215\n",
            "  10.73046102  8.83071611 14.91113578  6.57685327 12.87923831  0.\n",
            "  15.46743773 10.96439702 15.88550779  6.22915001 15.86922793  7.55467502\n",
            "  12.88127075  4.89848822]\n",
            " [16.59867848 16.47124698 10.8161436  10.99741633 16.74853586 16.49947996\n",
            "   7.16585263  8.42572334  1.10703241 17.7487424   2.6965209  15.46743773\n",
            "   0.          5.23921223 11.14290148 10.61441589  5.07705839 12.86623666\n",
            "   9.85732194 18.8310735 ]\n",
            " [14.27413627 13.01868887  8.3006773  13.21094063 12.48424171 11.54335036\n",
            "   2.33328847  3.19223347  4.34892828 14.66836457  3.6237403  10.96439702\n",
            "   5.23921223  0.         12.84269954  7.65567831  8.4783787   7.64616265\n",
            "  10.44457358 15.00479059]\n",
            " [11.04137357 13.60851248  8.99732144  1.27469301 16.01936434 18.38186105\n",
            "  15.07301839 14.89749402 11.87430471 13.55014187  9.90750697 15.88550779\n",
            "  11.14290148 12.84269954  0.          9.83380566  6.33603062 18.46876779\n",
            "   3.05529854 16.25007203]\n",
            " [ 6.62991984  5.86544785  1.01600215 10.84891727  6.83215712  8.56067924\n",
            "   8.78716436  7.49228063 10.3940824   7.1960052   7.92191087  6.22915001\n",
            "  10.61441589  7.65567831  9.83380566  0.          9.87234526  9.52433227\n",
            "   6.78329149  8.33007211]\n",
            " [14.33070509 15.4020818   9.57902657  5.99882853 16.68950867 17.66629151\n",
            "  10.79849987 11.36067587  6.00052757 16.15616614  4.88286527 15.86922793\n",
            "   5.07705839  8.4783787   6.33603062  9.87234526  0.         15.65524355\n",
            "   5.96237406 18.03169578]\n",
            " [14.8669568  11.77159113 10.53976965 19.20073763  9.35281869  6.12402049\n",
            "   5.87769296  4.45728623 11.90934035 13.80271508 11.08398952  7.55467502\n",
            "  12.86623666  7.64616265 18.46876779  9.52433227 15.65524355  0.\n",
            "  15.52943181 12.44893289]\n",
            " [ 8.88269831 10.91724302  5.96881341  4.0916658  13.10836514 15.33858493\n",
            "  12.55375999 12.16228168 10.35796599 11.13529619  8.02657554 12.88127075\n",
            "   9.85732194 10.44457358  3.05529854  6.78329149  5.96237406 15.52943181\n",
            "   0.         13.59645093]\n",
            " [ 5.93568556  2.68631492  8.4594488  17.48364466  3.09728046  7.01000059\n",
            "  15.20649453 13.37487057 18.48213623  3.20918391 16.1384518   4.89848822\n",
            "  18.8310735  15.00479059 16.25007203  8.33007211 18.03169578 12.44893289\n",
            "  13.59645093  0.        ]]\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "after 1st reset\n",
            "ld  [1 2 6 2 8 2 4 7 1 5 3 3 2 4 5 7 3 6 3 8]\n",
            "ld  [0 2 6 2 8 2 4 7 1 5 3 3 2 4 5 7 3 6 3 8]\n",
            "[[ 0.          3.95198098  6.01052114 12.31455169  7.30384382 11.1883957\n",
            "  15.36308519 13.91485782 16.59536929  2.76940835 14.01206717  8.33708092\n",
            "  16.59867848 14.27413627 11.04137357  6.62991984 14.33070509 14.8669568\n",
            "   8.88269831  5.93568556]\n",
            " [ 3.95198098  0.          5.86654744 14.83201044  3.36218314  7.35889893\n",
            "  13.54012009 11.82303699 16.20482995  2.03323992 13.77558707  4.59614396\n",
            "  16.47124698 13.01868887 13.60851248  5.86544785 15.4020818  11.77159113\n",
            "  10.91724302  2.68631492]\n",
            " [ 6.01052114  5.86654744  0.         10.05775251  7.31312299  9.39084217\n",
            "   9.59623065  8.39404245 10.69708633  6.94087633  8.15474887  6.9216408\n",
            "  10.8161436   8.3006773   8.99732144  1.01600215  9.57902657 10.53976965\n",
            "   5.96881341  8.4594488 ]\n",
            " [12.31455169 14.83201044 10.05775251  0.         17.17292312 19.40959616\n",
            "  15.49187863 15.46418223 11.8163843  14.81219171 10.06118069 16.97286589\n",
            "  10.99741633 13.21094063  1.27469301 10.84891727  5.99882853 19.20073763\n",
            "   4.0916658  17.48364466]\n",
            " [ 7.30384382  3.36218314  7.31312299 17.17292312  0.          4.12121152\n",
            "  12.41737791 10.53740445 16.26913679  5.1593172  14.10325725  1.81443808\n",
            "  16.74853586 12.48424171 16.01936434  6.83215712 16.68950867  9.35281869\n",
            "  13.10836514  3.09728046]\n",
            " [11.1883957   7.35889893  9.39084217 19.40959616  4.12121152  0.\n",
            "  10.71765502  8.82279823 15.78628045  9.25859306 14.08948968  2.87610215\n",
            "  16.49947996 11.54335036 18.38186105  8.56067924 17.66629151  6.12402049\n",
            "  15.33858493  7.01000059]\n",
            " [15.36308519 13.54012009  9.59623065 15.49187863 12.41737791 10.71765502\n",
            "   0.          1.91844727  6.14012873 15.35789091  5.92437391 10.73046102\n",
            "   7.16585263  2.33328847 15.07301839  8.78716436 10.79849987  5.87769296\n",
            "  12.55375999 15.20649453]\n",
            " [13.91485782 11.82303699  8.39404245 15.46418223 10.53740445  8.82279823\n",
            "   1.91844727  0.          7.49908512 13.69780528  6.6634543   8.83071611\n",
            "   8.42572334  3.19223347 14.89749402  7.49228063 11.36067587  4.45728623\n",
            "  12.16228168 13.37487057]\n",
            " [16.59536929 16.20482995 10.69708633 11.8163843  16.26913679 15.78628045\n",
            "   6.14012873  7.49908512  0.         17.58073609  2.59588561 14.91113578\n",
            "   1.10703241  4.34892828 11.87430471 10.3940824   6.00052757 11.90934035\n",
            "  10.35796599 18.48213623]\n",
            " [ 2.76940835  2.03323992  6.94087633 14.81219171  5.1593172   9.25859306\n",
            "  15.35789091 13.69780528 17.58073609  0.         15.07344853  6.57685327\n",
            "  17.7487424  14.66836457 13.55014187  7.1960052  16.15616614 13.80271508\n",
            "  11.13529619  3.20918391]\n",
            " [14.01206717 13.77558707  8.15474887 10.06118069 14.10325725 14.08948968\n",
            "   5.92437391  6.6634543   2.59588561 15.07344853  0.         12.87923831\n",
            "   2.6965209   3.6237403   9.90750697  7.92191087  4.88286527 11.08398952\n",
            "   8.02657554 16.1384518 ]\n",
            " [ 8.33708092  4.59614396  6.9216408  16.97286589  1.81443808  2.87610215\n",
            "  10.73046102  8.83071611 14.91113578  6.57685327 12.87923831  0.\n",
            "  15.46743773 10.96439702 15.88550779  6.22915001 15.86922793  7.55467502\n",
            "  12.88127075  4.89848822]\n",
            " [16.59867848 16.47124698 10.8161436  10.99741633 16.74853586 16.49947996\n",
            "   7.16585263  8.42572334  1.10703241 17.7487424   2.6965209  15.46743773\n",
            "   0.          5.23921223 11.14290148 10.61441589  5.07705839 12.86623666\n",
            "   9.85732194 18.8310735 ]\n",
            " [14.27413627 13.01868887  8.3006773  13.21094063 12.48424171 11.54335036\n",
            "   2.33328847  3.19223347  4.34892828 14.66836457  3.6237403  10.96439702\n",
            "   5.23921223  0.         12.84269954  7.65567831  8.4783787   7.64616265\n",
            "  10.44457358 15.00479059]\n",
            " [11.04137357 13.60851248  8.99732144  1.27469301 16.01936434 18.38186105\n",
            "  15.07301839 14.89749402 11.87430471 13.55014187  9.90750697 15.88550779\n",
            "  11.14290148 12.84269954  0.          9.83380566  6.33603062 18.46876779\n",
            "   3.05529854 16.25007203]\n",
            " [ 6.62991984  5.86544785  1.01600215 10.84891727  6.83215712  8.56067924\n",
            "   8.78716436  7.49228063 10.3940824   7.1960052   7.92191087  6.22915001\n",
            "  10.61441589  7.65567831  9.83380566  0.          9.87234526  9.52433227\n",
            "   6.78329149  8.33007211]\n",
            " [14.33070509 15.4020818   9.57902657  5.99882853 16.68950867 17.66629151\n",
            "  10.79849987 11.36067587  6.00052757 16.15616614  4.88286527 15.86922793\n",
            "   5.07705839  8.4783787   6.33603062  9.87234526  0.         15.65524355\n",
            "   5.96237406 18.03169578]\n",
            " [14.8669568  11.77159113 10.53976965 19.20073763  9.35281869  6.12402049\n",
            "   5.87769296  4.45728623 11.90934035 13.80271508 11.08398952  7.55467502\n",
            "  12.86623666  7.64616265 18.46876779  9.52433227 15.65524355  0.\n",
            "  15.52943181 12.44893289]\n",
            " [ 8.88269831 10.91724302  5.96881341  4.0916658  13.10836514 15.33858493\n",
            "  12.55375999 12.16228168 10.35796599 11.13529619  8.02657554 12.88127075\n",
            "   9.85732194 10.44457358  3.05529854  6.78329149  5.96237406 15.52943181\n",
            "   0.         13.59645093]\n",
            " [ 5.93568556  2.68631492  8.4594488  17.48364466  3.09728046  7.01000059\n",
            "  15.20649453 13.37487057 18.48213623  3.20918391 16.1384518   4.89848822\n",
            "  18.8310735  15.00479059 16.25007203  8.33007211 18.03169578 12.44893289\n",
            "  13.59645093  0.        ]]\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "after 1st reset\n",
            "ld  [6 2 7 8 6 3 4 2 8 1 8 1 1 1 2 4 3 4 4 7]\n",
            "ld  [0 2 7 8 6 3 4 2 8 1 8 1 1 1 2 4 3 4 4 7]\n",
            "[[ 0.          3.95198098  6.01052114 12.31455169  7.30384382 11.1883957\n",
            "  15.36308519 13.91485782 16.59536929  2.76940835 14.01206717  8.33708092\n",
            "  16.59867848 14.27413627 11.04137357  6.62991984 14.33070509 14.8669568\n",
            "   8.88269831  5.93568556]\n",
            " [ 3.95198098  0.          5.86654744 14.83201044  3.36218314  7.35889893\n",
            "  13.54012009 11.82303699 16.20482995  2.03323992 13.77558707  4.59614396\n",
            "  16.47124698 13.01868887 13.60851248  5.86544785 15.4020818  11.77159113\n",
            "  10.91724302  2.68631492]\n",
            " [ 6.01052114  5.86654744  0.         10.05775251  7.31312299  9.39084217\n",
            "   9.59623065  8.39404245 10.69708633  6.94087633  8.15474887  6.9216408\n",
            "  10.8161436   8.3006773   8.99732144  1.01600215  9.57902657 10.53976965\n",
            "   5.96881341  8.4594488 ]\n",
            " [12.31455169 14.83201044 10.05775251  0.         17.17292312 19.40959616\n",
            "  15.49187863 15.46418223 11.8163843  14.81219171 10.06118069 16.97286589\n",
            "  10.99741633 13.21094063  1.27469301 10.84891727  5.99882853 19.20073763\n",
            "   4.0916658  17.48364466]\n",
            " [ 7.30384382  3.36218314  7.31312299 17.17292312  0.          4.12121152\n",
            "  12.41737791 10.53740445 16.26913679  5.1593172  14.10325725  1.81443808\n",
            "  16.74853586 12.48424171 16.01936434  6.83215712 16.68950867  9.35281869\n",
            "  13.10836514  3.09728046]\n",
            " [11.1883957   7.35889893  9.39084217 19.40959616  4.12121152  0.\n",
            "  10.71765502  8.82279823 15.78628045  9.25859306 14.08948968  2.87610215\n",
            "  16.49947996 11.54335036 18.38186105  8.56067924 17.66629151  6.12402049\n",
            "  15.33858493  7.01000059]\n",
            " [15.36308519 13.54012009  9.59623065 15.49187863 12.41737791 10.71765502\n",
            "   0.          1.91844727  6.14012873 15.35789091  5.92437391 10.73046102\n",
            "   7.16585263  2.33328847 15.07301839  8.78716436 10.79849987  5.87769296\n",
            "  12.55375999 15.20649453]\n",
            " [13.91485782 11.82303699  8.39404245 15.46418223 10.53740445  8.82279823\n",
            "   1.91844727  0.          7.49908512 13.69780528  6.6634543   8.83071611\n",
            "   8.42572334  3.19223347 14.89749402  7.49228063 11.36067587  4.45728623\n",
            "  12.16228168 13.37487057]\n",
            " [16.59536929 16.20482995 10.69708633 11.8163843  16.26913679 15.78628045\n",
            "   6.14012873  7.49908512  0.         17.58073609  2.59588561 14.91113578\n",
            "   1.10703241  4.34892828 11.87430471 10.3940824   6.00052757 11.90934035\n",
            "  10.35796599 18.48213623]\n",
            " [ 2.76940835  2.03323992  6.94087633 14.81219171  5.1593172   9.25859306\n",
            "  15.35789091 13.69780528 17.58073609  0.         15.07344853  6.57685327\n",
            "  17.7487424  14.66836457 13.55014187  7.1960052  16.15616614 13.80271508\n",
            "  11.13529619  3.20918391]\n",
            " [14.01206717 13.77558707  8.15474887 10.06118069 14.10325725 14.08948968\n",
            "   5.92437391  6.6634543   2.59588561 15.07344853  0.         12.87923831\n",
            "   2.6965209   3.6237403   9.90750697  7.92191087  4.88286527 11.08398952\n",
            "   8.02657554 16.1384518 ]\n",
            " [ 8.33708092  4.59614396  6.9216408  16.97286589  1.81443808  2.87610215\n",
            "  10.73046102  8.83071611 14.91113578  6.57685327 12.87923831  0.\n",
            "  15.46743773 10.96439702 15.88550779  6.22915001 15.86922793  7.55467502\n",
            "  12.88127075  4.89848822]\n",
            " [16.59867848 16.47124698 10.8161436  10.99741633 16.74853586 16.49947996\n",
            "   7.16585263  8.42572334  1.10703241 17.7487424   2.6965209  15.46743773\n",
            "   0.          5.23921223 11.14290148 10.61441589  5.07705839 12.86623666\n",
            "   9.85732194 18.8310735 ]\n",
            " [14.27413627 13.01868887  8.3006773  13.21094063 12.48424171 11.54335036\n",
            "   2.33328847  3.19223347  4.34892828 14.66836457  3.6237403  10.96439702\n",
            "   5.23921223  0.         12.84269954  7.65567831  8.4783787   7.64616265\n",
            "  10.44457358 15.00479059]\n",
            " [11.04137357 13.60851248  8.99732144  1.27469301 16.01936434 18.38186105\n",
            "  15.07301839 14.89749402 11.87430471 13.55014187  9.90750697 15.88550779\n",
            "  11.14290148 12.84269954  0.          9.83380566  6.33603062 18.46876779\n",
            "   3.05529854 16.25007203]\n",
            " [ 6.62991984  5.86544785  1.01600215 10.84891727  6.83215712  8.56067924\n",
            "   8.78716436  7.49228063 10.3940824   7.1960052   7.92191087  6.22915001\n",
            "  10.61441589  7.65567831  9.83380566  0.          9.87234526  9.52433227\n",
            "   6.78329149  8.33007211]\n",
            " [14.33070509 15.4020818   9.57902657  5.99882853 16.68950867 17.66629151\n",
            "  10.79849987 11.36067587  6.00052757 16.15616614  4.88286527 15.86922793\n",
            "   5.07705839  8.4783787   6.33603062  9.87234526  0.         15.65524355\n",
            "   5.96237406 18.03169578]\n",
            " [14.8669568  11.77159113 10.53976965 19.20073763  9.35281869  6.12402049\n",
            "   5.87769296  4.45728623 11.90934035 13.80271508 11.08398952  7.55467502\n",
            "  12.86623666  7.64616265 18.46876779  9.52433227 15.65524355  0.\n",
            "  15.52943181 12.44893289]\n",
            " [ 8.88269831 10.91724302  5.96881341  4.0916658  13.10836514 15.33858493\n",
            "  12.55375999 12.16228168 10.35796599 11.13529619  8.02657554 12.88127075\n",
            "   9.85732194 10.44457358  3.05529854  6.78329149  5.96237406 15.52943181\n",
            "   0.         13.59645093]\n",
            " [ 5.93568556  2.68631492  8.4594488  17.48364466  3.09728046  7.01000059\n",
            "  15.20649453 13.37487057 18.48213623  3.20918391 16.1384518   4.89848822\n",
            "  18.8310735  15.00479059 16.25007203  8.33007211 18.03169578 12.44893289\n",
            "  13.59645093  0.        ]]\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "after 1st reset\n"
          ]
        }
      ],
      "source": [
        "env = VRPenvironement2(N,M,customers,coords,Loads,max_capacity)\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "train_py_env = VRPenvironement2(N,M,customers,coords,Loads,max_capacity)\n",
        "eval_py_env = VRPenvironement2(N,M,customers,coords,Loads,max_capacity)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "\n",
        "#env.reset()\n",
        "#env.step(2)\n",
        "\n",
        "#print(splitter_fn(env._observation_spec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LplxJIAehMzp",
        "outputId": "d511767a-1c22-4cbe-e630-2975496217e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ArraySpec(shape=(14,), dtype=dtype('int32'), name='observations')\n",
            "TensorSpec(shape=(14,), dtype=tf.int32, name='observations')\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_spec1())\n",
        "print(from_spec(env.observation_spec1()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iipoaeSxOiF8"
      },
      "source": [
        "# **Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os1aI66FOboH",
        "outputId": "a9e98bb8-09a9-4237-824d-66fe93f1e0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "print(action_tensor_spec)\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "print(num_actions)\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=3.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "#input_layer = tf.keras.layers.Dense(100, name=\"input\",input_shape=(14,))\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    #observation_spec['observations'],\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.06, maxval=0.06),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))#,train_env.observation_spec()['observations'], train_env.action_spec(), name= 'q_net'\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer], train_env.observation_spec()['observations'])#, from_spec(env.observation_spec1())\n",
        "\n",
        "#qq_net = MaskSplitterNetwork(splitter_fn, q_net, train_env.observation_spec(), passthrough_mask=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EDVr4PuPFGu"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "#train_step_counter = tf.Variable(0)\n",
        "train_step_counter = tf.compat.v2.Variable(0)\n",
        "#train_step_counter = tf.Variable(0, dtype=tf.int64)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    epsilon_greedy = epsilon_greedyp,\n",
        "    #gamma = 0.2,\n",
        "    ##observation_and_action_constraint_splitter = train_env.observation_and_action_constraint_splitter(),\n",
        "    observation_and_action_constraint_splitter = splitter_fn,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss, #common.element_wise_huber_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTuJAM34PH1A"
      },
      "source": [
        "# **Policies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb7IMUSQPJNI"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy #used for evaluation and deployment.\n",
        "collect_policy = agent.collect_policy #used for data collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rERkzGkPNDh"
      },
      "outputs": [],
      "source": [
        "#randomly select an action for each time_step\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjDqXcb9PP1U",
        "outputId": "78f8c6c2-e0bf-43e5-82a5-4f3abe6ffbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ld  [5 8 5 6 4 3 6 5 4 8 2 8 2 4 6 3 6 8 8 3]\n",
            "ld  [0 8 5 6 4 3 6 5 4 8 2 8 2 4 6 3 6 8 8 3]\n",
            "[[ 0.          3.95198098  6.01052114 12.31455169  7.30384382 11.1883957\n",
            "  15.36308519 13.91485782 16.59536929  2.76940835 14.01206717  8.33708092\n",
            "  16.59867848 14.27413627 11.04137357  6.62991984 14.33070509 14.8669568\n",
            "   8.88269831  5.93568556]\n",
            " [ 3.95198098  0.          5.86654744 14.83201044  3.36218314  7.35889893\n",
            "  13.54012009 11.82303699 16.20482995  2.03323992 13.77558707  4.59614396\n",
            "  16.47124698 13.01868887 13.60851248  5.86544785 15.4020818  11.77159113\n",
            "  10.91724302  2.68631492]\n",
            " [ 6.01052114  5.86654744  0.         10.05775251  7.31312299  9.39084217\n",
            "   9.59623065  8.39404245 10.69708633  6.94087633  8.15474887  6.9216408\n",
            "  10.8161436   8.3006773   8.99732144  1.01600215  9.57902657 10.53976965\n",
            "   5.96881341  8.4594488 ]\n",
            " [12.31455169 14.83201044 10.05775251  0.         17.17292312 19.40959616\n",
            "  15.49187863 15.46418223 11.8163843  14.81219171 10.06118069 16.97286589\n",
            "  10.99741633 13.21094063  1.27469301 10.84891727  5.99882853 19.20073763\n",
            "   4.0916658  17.48364466]\n",
            " [ 7.30384382  3.36218314  7.31312299 17.17292312  0.          4.12121152\n",
            "  12.41737791 10.53740445 16.26913679  5.1593172  14.10325725  1.81443808\n",
            "  16.74853586 12.48424171 16.01936434  6.83215712 16.68950867  9.35281869\n",
            "  13.10836514  3.09728046]\n",
            " [11.1883957   7.35889893  9.39084217 19.40959616  4.12121152  0.\n",
            "  10.71765502  8.82279823 15.78628045  9.25859306 14.08948968  2.87610215\n",
            "  16.49947996 11.54335036 18.38186105  8.56067924 17.66629151  6.12402049\n",
            "  15.33858493  7.01000059]\n",
            " [15.36308519 13.54012009  9.59623065 15.49187863 12.41737791 10.71765502\n",
            "   0.          1.91844727  6.14012873 15.35789091  5.92437391 10.73046102\n",
            "   7.16585263  2.33328847 15.07301839  8.78716436 10.79849987  5.87769296\n",
            "  12.55375999 15.20649453]\n",
            " [13.91485782 11.82303699  8.39404245 15.46418223 10.53740445  8.82279823\n",
            "   1.91844727  0.          7.49908512 13.69780528  6.6634543   8.83071611\n",
            "   8.42572334  3.19223347 14.89749402  7.49228063 11.36067587  4.45728623\n",
            "  12.16228168 13.37487057]\n",
            " [16.59536929 16.20482995 10.69708633 11.8163843  16.26913679 15.78628045\n",
            "   6.14012873  7.49908512  0.         17.58073609  2.59588561 14.91113578\n",
            "   1.10703241  4.34892828 11.87430471 10.3940824   6.00052757 11.90934035\n",
            "  10.35796599 18.48213623]\n",
            " [ 2.76940835  2.03323992  6.94087633 14.81219171  5.1593172   9.25859306\n",
            "  15.35789091 13.69780528 17.58073609  0.         15.07344853  6.57685327\n",
            "  17.7487424  14.66836457 13.55014187  7.1960052  16.15616614 13.80271508\n",
            "  11.13529619  3.20918391]\n",
            " [14.01206717 13.77558707  8.15474887 10.06118069 14.10325725 14.08948968\n",
            "   5.92437391  6.6634543   2.59588561 15.07344853  0.         12.87923831\n",
            "   2.6965209   3.6237403   9.90750697  7.92191087  4.88286527 11.08398952\n",
            "   8.02657554 16.1384518 ]\n",
            " [ 8.33708092  4.59614396  6.9216408  16.97286589  1.81443808  2.87610215\n",
            "  10.73046102  8.83071611 14.91113578  6.57685327 12.87923831  0.\n",
            "  15.46743773 10.96439702 15.88550779  6.22915001 15.86922793  7.55467502\n",
            "  12.88127075  4.89848822]\n",
            " [16.59867848 16.47124698 10.8161436  10.99741633 16.74853586 16.49947996\n",
            "   7.16585263  8.42572334  1.10703241 17.7487424   2.6965209  15.46743773\n",
            "   0.          5.23921223 11.14290148 10.61441589  5.07705839 12.86623666\n",
            "   9.85732194 18.8310735 ]\n",
            " [14.27413627 13.01868887  8.3006773  13.21094063 12.48424171 11.54335036\n",
            "   2.33328847  3.19223347  4.34892828 14.66836457  3.6237403  10.96439702\n",
            "   5.23921223  0.         12.84269954  7.65567831  8.4783787   7.64616265\n",
            "  10.44457358 15.00479059]\n",
            " [11.04137357 13.60851248  8.99732144  1.27469301 16.01936434 18.38186105\n",
            "  15.07301839 14.89749402 11.87430471 13.55014187  9.90750697 15.88550779\n",
            "  11.14290148 12.84269954  0.          9.83380566  6.33603062 18.46876779\n",
            "   3.05529854 16.25007203]\n",
            " [ 6.62991984  5.86544785  1.01600215 10.84891727  6.83215712  8.56067924\n",
            "   8.78716436  7.49228063 10.3940824   7.1960052   7.92191087  6.22915001\n",
            "  10.61441589  7.65567831  9.83380566  0.          9.87234526  9.52433227\n",
            "   6.78329149  8.33007211]\n",
            " [14.33070509 15.4020818   9.57902657  5.99882853 16.68950867 17.66629151\n",
            "  10.79849987 11.36067587  6.00052757 16.15616614  4.88286527 15.86922793\n",
            "   5.07705839  8.4783787   6.33603062  9.87234526  0.         15.65524355\n",
            "   5.96237406 18.03169578]\n",
            " [14.8669568  11.77159113 10.53976965 19.20073763  9.35281869  6.12402049\n",
            "   5.87769296  4.45728623 11.90934035 13.80271508 11.08398952  7.55467502\n",
            "  12.86623666  7.64616265 18.46876779  9.52433227 15.65524355  0.\n",
            "  15.52943181 12.44893289]\n",
            " [ 8.88269831 10.91724302  5.96881341  4.0916658  13.10836514 15.33858493\n",
            "  12.55375999 12.16228168 10.35796599 11.13529619  8.02657554 12.88127075\n",
            "   9.85732194 10.44457358  3.05529854  6.78329149  5.96237406 15.52943181\n",
            "   0.         13.59645093]\n",
            " [ 5.93568556  2.68631492  8.4594488  17.48364466  3.09728046  7.01000059\n",
            "  15.20649453 13.37487057 18.48213623  3.20918391 16.1384518   4.89848822\n",
            "  18.8310735  15.00479059 16.25007203  8.33007211 18.03169578 12.44893289\n",
            "  13.59645093  0.        ]]\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "after 1st reset\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "q:  (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "random\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  4\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "q:  (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.46144444,  0.04645929, -0.10953054, -0.49853146,  0.01307113,\n",
            "        -0.4687693 ,  0.08403103, -0.27408317, -0.04010862, -1.0066472 ,\n",
            "        -0.6643211 , -0.3558649 , -0.4776644 , -0.5467794 , -0.0760994 ,\n",
            "        -0.6455479 , -0.8884792 ,  0.01838769, -0.6090513 ]],\n",
            "      dtype=float32)>, ())\n",
            "policy\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  34.85490789552589\n",
            "ending episode1 False\n",
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([34.854908], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "q:  (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.6727709 , -0.55692595,  0.89761883, -1.4899335 ,  0.04407641,\n",
            "        -0.9273123 ,  0.7462717 ,  0.5077476 ,  0.64925027, -1.5020015 ,\n",
            "        -0.9465239 , -1.0330409 , -0.14297403, -0.40593064, -0.2942467 ,\n",
            "        -0.40041417,  0.5889373 , -0.5327348 , -1.1590449 ]],\n",
            "      dtype=float32)>, ())\n",
            "random\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  10\n",
            "gr:  73.82142614990343\n",
            "ending episode1 False\n",
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  8, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True, False,  True,  True,\n",
            "         True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([73.82143], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "q:  (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.9459295 , -2.024404  , -1.2767768 , -4.233792  ,  0.22745089,\n",
            "        -0.9730745 , -0.10857598,  0.5742158 ,  0.76674426, -3.0581582 ,\n",
            "        -1.9033006 , -0.6358041 ,  1.1087391 , -0.01691757,  0.5931403 ,\n",
            "         1.50979   ,  0.79145795, -0.5802455 , -0.7362301 ]],\n",
            "      dtype=float32)>, ())\n",
            "policy\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  16\n",
            "gr:  124.30956570573434\n",
            "ending episode1 False\n",
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  8, 14, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True, False,  True,  True,\n",
            "         True,  True,  True, False,  True,  True, False,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([124.30956], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "q:  (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.4697864 , -2.2726984 , -1.2377069 , -1.1267407 ,  1.8696077 ,\n",
            "         1.6743497 ,  0.10484426,  0.92077595,  1.2496173 , -3.3267586 ,\n",
            "        -3.1469188 , -2.1343596 ,  1.1563265 ,  1.5118513 ,  1.2800189 ,\n",
            "        -1.3677809 ,  0.42937386,  0.3818764 , -2.0806882 ]],\n",
            "      dtype=float32)>, ())\n"
          ]
        }
      ],
      "source": [
        "env1 = VRPenvironement2(N,M,customers,coords,Loads,max_capacity)\n",
        "tf_env1 = tf_py_environment.TFPyEnvironment(env)\n",
        "time_step = tf_env1.reset()\n",
        "print(\"q: \", q_net(time_step.observation['observations']))\n",
        "print(\"random\")\n",
        "c = random_policy.action(time_step)\n",
        "time_step = tf_env1.step(c)\n",
        "print(\"q: \", q_net(time_step.observation['observations']))\n",
        "print(\"policy\")\n",
        "c = eval_policy.action(time_step)\n",
        "time_step = tf_env1.step(c)\n",
        "print(time_step)\n",
        "print(\"q: \", q_net(time_step.observation['observations']))\n",
        "print(\"random\")\n",
        "c = random_policy.action(time_step)\n",
        "time_step = tf_env1.step(c)\n",
        "print(time_step)\n",
        "print(\"q: \", q_net(time_step.observation['observations']))\n",
        "print(\"policy\")\n",
        "c = eval_policy.action(time_step)\n",
        "time_step = tf_env1.step(c)\n",
        "print(time_step)\n",
        "print(\"q: \", q_net(time_step.observation['observations']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu17opBIX0oj",
        "outputId": "46ad5193-f469-4481-8d8b-f3a2a2d493f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorSpec(shape=(14,), dtype=tf.int32, name='observations')\n"
          ]
        }
      ],
      "source": [
        "#print(qq_net.output_spec)\n",
        "#print(distribution(train_env.current_time_step()))\n",
        "#print(collect_policy.distribution(train_env.current_time_step()).action.log_prob((0,1,2,3,4)).numpy())\n",
        "#print(q_net.input)\n",
        "#print(q_net.input_spec)\n",
        "print(q_net.input_tensor_spec)\n",
        "#print(q_net.input_mask)\n",
        "#print(q_net.input_mask_at)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwW_EAPLPTgK"
      },
      "source": [
        "# **Metrics and Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7zFr_k7PQmN"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    print(\"********Episode************* \",_)\n",
        "    time_step = environment.reset()\n",
        "    print(\"time_step\", time_step)\n",
        "    episode_return = 0.0\n",
        "    i = 0\n",
        "    print(\"start while loop\")\n",
        "    while not time_step.is_last():\n",
        "      print(\"@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\")\n",
        "\n",
        "      print(\"obs: \", time_step.observation)\n",
        "      print(\"qnet_obs\", q_net(time_step.observation['observations']))\n",
        "      #print(\"qnet_obs0\", q_net(time_step.observation)[0])\n",
        "\n",
        "      #d = tfp.distributions.Categorical(q_net(time_step.observation['observations']))\n",
        "      #print(\"dtfp \", d)\n",
        "      #print(\"dmode \", d.mode())\n",
        "      #print(\"here \", (time_step.observation).numpy() [0][1])\n",
        "      print(\"nact: \", policy.action_spec)\n",
        "      #if(i == 0):\n",
        "      action_step = policy.action(time_step)\n",
        "      print(\"action_step: \", action_step)\n",
        "      print(\"action_step0 \", (action_step[0]).numpy()[0]) #action_step.action\n",
        "      #else:\n",
        "      if(i != 0):\n",
        "        b = 1\n",
        "        while b == 1:\n",
        "          b = 0\n",
        "          #print(\"a\", (action_step[0]).numpy()[0])\n",
        "          for c in (time_step.observation['observations']).numpy() [0][:]:\n",
        "            #print(\"b1\", c)\n",
        "            #print(\"b2\", (action_step[0]).numpy()[0])\n",
        "            if c == ((action_step[0]).numpy()[0] + 2):\n",
        "              #print(\"c\")\n",
        "              b = 1\n",
        "\n",
        "          if b == 1:\n",
        "            #tn = (q_net(time_step.observation)[0]).numpy()\n",
        "            #tn = tn.numpy()\n",
        "            #tn[0,c] = -100\n",
        "            #print(\"tn \", tn)\n",
        "            #tn = tf.Variable(q_net(time_step.observation)[0])\n",
        "            #print(\"tn \", tn)\n",
        "            #print(\"c \", int((action_step[0]).numpy()[0] + 2))\n",
        "            #print(\"ff \", tn[0,int((action_step[0]).numpy()[0])])\n",
        "            #tn[0,int((action_step[0]).numpy()[0])].assign(-10)\n",
        "            #print(\"tn \", tn)\n",
        "            #print(\"network-1: \", q_net.variables)\n",
        "            #print(\"type of: \", (q_net(time_step.observation)[0]).dtype)\n",
        "            #q_net(time_step.observation)[0] = tuple(tf.constant(tn))\n",
        "\n",
        "            print(\"&&&&&&&&&&&&&&&&&&&&&&&&&&\")\n",
        "            #action_step = policy.action(time_step)\n",
        "            action_step = random_policy.action(time_step)\n",
        "            print(\"new action \", (action_step[0]).numpy()[0])\n",
        "\n",
        "      i = i + 1\n",
        "      #print(\"distribution \", policy.distribution(time_step))\n",
        "      #qnet_obs = q_net(time_step.observation)\n",
        "      #qnet_obs[1,action_step] = -100\n",
        "      time_step = environment.step(action_step)\n",
        "      print(\"time_step \", time_step)\n",
        "      print(\"distribution \", policy.distribution(time_step))\n",
        "      episode_return = episode_return + time_step.reward\n",
        "      print(\"episode_return \", episode_return)\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keL5no1HPZy3",
        "outputId": "23bcbfa1-3d96-4ecd-834b-e12c3089493a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  3\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  4\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  5\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  6\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  7\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  8\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n",
            "********Episode*************  9\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2,\n",
            "        -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  2\n",
            "gr:  16.048019021962993\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.04802], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([16.04802], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.31506866, -0.10229915, -0.09934734, -0.29238772, -0.09121606,\n",
            "        -0.32089442, -0.05244882, -0.20004627, -0.16610502, -0.522208  ,\n",
            "        -0.43930408, -0.3246496 , -0.34755832, -0.37398094, -0.15596727,\n",
            "        -0.44041407, -0.52407205, -0.10045467, -0.40240008]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  2\n",
            "1Loaded_weight+  4\n",
            "gr:  40.273001057424025\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.273003], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([56.321022], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.4375662 , -0.35105765,  0.9469138 , -1.3804592 ,  0.09184588,\n",
            "        -0.95642835,  0.6356885 ,  0.44224066,  0.66444886, -1.2585407 ,\n",
            "        -0.6323193 , -0.94143486, -0.12896292, -0.42233598, -0.3004967 ,\n",
            "        -0.35577506,  0.6338201 , -0.74922717, -1.1137743 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  12\n",
            "gr:  69.03380086786103\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([69.0338], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([125.35482], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 2, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-1.5247275 , -0.8948107 ,  0.17840974, -2.105654  , -0.17539233,\n",
            "        -0.8850169 ,  0.76155573,  0.4885779 ,  0.67947567, -1.7757815 ,\n",
            "        -0.86079055, -0.74285495,  0.4439928 , -0.4090442 ,  0.01389855,\n",
            "         0.24712972,  1.177427  , -0.7805348 , -1.1639355 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  16\n",
            "gr:  98.59386305274194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.593864], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([223.94868], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.61101013, -0.20134184,  0.5565581 ,  0.4526412 ,  1.8751354 ,\n",
            "         1.0504928 , -0.34597778,  1.3324913 ,  1.0289553 , -2.1243029 ,\n",
            "        -3.317814  , -2.772156  ,  0.78123695,  0.8644565 ,  0.5432324 ,\n",
            "        -2.2569013 ,  0.37080628,  0.40985507, -2.7669425 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  19\n",
            "gr:  142.02990475156932\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([142.0299], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([365.97858], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.4222504 ,  0.01356804,  0.9641867 ,  0.42535728,  2.2583246 ,\n",
            "         0.66886926, -0.18535274,  1.0645113 ,  1.0290389 , -2.6890755 ,\n",
            "        -3.683987  , -2.5272295 ,  0.4686013 ,  0.8359887 , -0.47520185,\n",
            "        -2.4783323 ,  0.22099994, -0.06199798, -2.3410332 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  188.87057716416933\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([188.87057], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([554.8491], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-6.8318033e-01,  2.6389956e-05,  1.3114185e+00, -9.6833557e-01,\n",
            "         1.0577874e-01, -7.6218955e-02,  6.7286044e-01,  1.0459756e+00,\n",
            "         1.0122484e+00, -1.7656373e+00, -3.5272875e+00, -3.4620001e+00,\n",
            "         1.8176517e+00,  8.7926799e-01, -4.6201688e-01, -2.8203278e+00,\n",
            "         1.0005175e+00, -7.7101278e-01, -1.3648853e+00]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  251.3623212936527\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([251.36232], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([806.2114], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False,  True,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.5713477 ,  0.61684537,  0.7903797 , -2.5499642 , -1.1547508 ,\n",
            "        -0.27777433,  0.6758949 ,  1.4799168 ,  0.95749766, -0.3977663 ,\n",
            "        -3.2960796 , -2.4767742 ,  2.6774683 ,  0.6899236 , -1.8193557 ,\n",
            "        -3.7181795 , -1.7490145 , -2.5515466 ,  0.15513621]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  319.1857008483098\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([319.1857], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1125.3971], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[-0.03199054,  1.9048948 ,  0.66608834, -0.91765517, -0.69208694,\n",
            "        -0.4544533 ,  0.70441157,  2.8602524 ,  0.55560577, -1.179355  ,\n",
            "        -3.7154062 , -2.0053387 ,  2.274212  , -0.31967884, -2.668803  ,\n",
            "        -3.5378947 , -2.0914714 , -1.6500833 , -0.08197106]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  400.0682040748325\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([400.0682], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1525.4653], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 0.2762043 ,  2.2371144 ,  0.26870382, -0.72093123, -0.8414161 ,\n",
            "        -0.24516213,  0.08894734,  2.6915805 ,  0.18865   , -1.4164618 ,\n",
            "        -3.542212  , -2.488377  ,  2.678629  , -0.14096695, -2.825039  ,\n",
            "        -3.6644866 , -2.2519991 , -1.7416264 ,  0.28375787]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  24\n",
            "gr:  492.49125849960194\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([492.49127], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2017.9565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False,  True, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 2.543247  ,  2.4392653 ,  2.3685074 ,  3.8024557 , -0.835252  ,\n",
            "         0.586186  , -0.47531468,  2.3635345 , -0.84294206, -0.7933426 ,\n",
            "        -3.2815447 , -3.384153  ,  2.4884079 ,  0.3921253 , -3.181109  ,\n",
            "        -7.313523  , -1.7840086 , -1.4397837 , -0.7690319 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  24\n",
            "add an other car\n",
            "Loaded_weight+  6\n",
            "gr:  597.6104690994282\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([597.6105], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2615.567], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False,  True, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.0507848 ,  2.5443804 ,  2.8698614 ,  3.922314  , -1.4037095 ,\n",
            "         0.04909246, -0.09887823,  2.193352  , -0.3722597 , -0.57275265,\n",
            "        -4.4244604 , -3.4450035 ,  1.9543386 , -0.14660248, -3.5503476 ,\n",
            "        -7.3040853 , -1.4357544 , -1.8963234 , -0.5773855 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  710.3123017852843\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([710.3123], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3325.8792], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.663015  ,  2.7867715 ,  3.5573063 ,  3.303449  , -2.076043  ,\n",
            "         1.1998612 ,  0.63650244,  2.813202  , -1.274464  , -0.7608414 ,\n",
            "        -4.3543878 , -4.375707  ,  1.9112909 ,  0.3227229 , -3.441226  ,\n",
            "        -6.885843  , -0.9528113 , -1.4192111 , -0.79755884]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  12\n",
            "gr:  827.9411160847428\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([827.9411], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4153.8203], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 5.5015335 ,  3.6087465 ,  3.870807  ,  2.3541808 , -1.4758317 ,\n",
            "        -0.1266412 ,  0.08846034,  5.719263  ,  0.0493267 ,  1.9661353 ,\n",
            "        -6.849374  , -4.8540344 ,  1.8989751 ,  0.6519907 , -5.7129917 ,\n",
            "        -5.699268  , -0.40472806, -3.3049304 ,  0.69669914]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  20\n",
            "gr:  955.6624234113078\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([955.6624], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5109.483], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 6.847846  ,  3.6394591 ,  3.2268565 ,  3.0630124 , -1.8212937 ,\n",
            "         0.40283757,  0.4768495 ,  4.296827  , -0.7555775 ,  0.84817797,\n",
            "        -6.1087775 , -3.2720602 ,  1.5764699 , -0.18324105, -5.4859204 ,\n",
            "        -5.6426845 , -0.7984115 , -3.3442867 ,  0.7406499 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  20\n",
            "1Loaded_weight+  21\n",
            "gr:  1100.6872098426002\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1100.6873], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6210.17], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.908235  ,  2.6718268 ,  1.285765  ,  1.7446935 , -1.9731115 ,\n",
            "         1.6069874 ,  1.4617705 ,  4.5151987 , -0.1468349 , -0.04689227,\n",
            "        -3.3267365 , -1.9465365 ,  2.8499303 ,  0.8456915 , -5.0606236 ,\n",
            "        -5.4483857 ,  0.3936231 , -3.5942967 , -0.91086113]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  21\n",
            "1Loaded_weight+  22\n",
            "gr:  1250.2445585452501\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1250.2445], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7460.4146], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.6421485 ,  3.4176242 ,  5.045416  ,  0.6433371 , -3.069145  ,\n",
            "         2.299996  ,  2.0040796 ,  4.917225  , -2.4427125 , -1.2794167 ,\n",
            "        -5.2481766 , -3.5558684 ,  3.9853928 , -0.06371881, -4.5310836 ,\n",
            "        -5.2734237 ,  0.2581424 , -3.3427825 ,  0.61727667]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1410.9192089427747\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1410.9192], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8871.334], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 4.466677  ,  6.586989  ,  6.1496024 ,  1.5998759 , -4.4782085 ,\n",
            "        -0.23673958,  4.572982  ,  2.1408143 ,  0.91731435, -0.43904108,\n",
            "        -6.309214  , -4.0143204 ,  2.0062604 , -0.9096218 , -4.0371857 ,\n",
            "        -4.0207534 ,  0.06521671, -5.005949  , -1.9128494 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1585.6314852801247\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1585.6315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10456.966], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[ 3.4614112 ,  5.214591  ,  6.5673537 ,  1.0822374 , -2.5395503 ,\n",
            "        -0.35066128,  1.393216  ,  3.4105234 ,  2.6717553 , -0.9959962 ,\n",
            "        -4.9226394 , -3.3414476 ,  2.353734  ,  0.2270162 , -2.9319453 ,\n",
            "        -2.5017555 ,  1.4315822 , -4.626665  , -1.4765439 ]],\n",
            "      dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  11\n",
            "gr:  1780.3437616174747\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  2,  8,  4, 18,  6,  1,  9, 14, 10,  3, 20,  1,  5,  7, 15,\n",
            "        11, 13, 12,  1, 19, 17, 16,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1780.3438], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12237.31], shape=(1,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12237.311"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "#compute_avg_return(eval_env, random_policy, num_eval_episodes)#a baseline performance in the environment.\n",
        "compute_avg_return(eval_env, eval_policy, num_eval_episodes)#a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nWzXIy0PeVv"
      },
      "source": [
        "# **Replay Buffer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n34W-TdWPdW_"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1), #N+2, N+M+1\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length= 2, #N + M + 1\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length= 2)#,N + M + 1\n",
        "  #stride_length=N+M+1,\n",
        "  #pad_end_of_episodes = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhuOxXdEPkyj",
        "outputId": "31aaa303-10c8-48ea-e38b-5fbe8e37bb17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': {'observations': TensorSpec(shape=(25,), dtype=tf.int32, name='observations'),\n",
              "                 'valid_actions': TensorSpec(shape=(19,), dtype=tf.bool, name='valid_actions')},\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "agent.collect_data_spec #containing the specs for observations, actions, rewards, and other items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERB0lhooPnh_",
        "outputId": "a18c6e51-81fa-4572-8f11-793fd4da6e31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0RcbPikPoGB"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "1sooHHxpPqis",
        "outputId": "1830cc26-b1d6-4354-9e16-c199229dfff3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-028f1608683b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m py_driver.PyDriver(\n\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     py_tf_eager_policy.PyTFEagerPolicy(\n\u001b[1;32m      5\u001b[0m       random_policy, use_tf_function=True),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_episodes = 1).run(train_py_env.reset()) #max_steps=initial_collect_steps\n",
        "\n",
        "#driver = py_driver.PyDriver(\n",
        "#    env,\n",
        "#    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "#      random_policy, use_tf_function=True),\n",
        "#    [rb_observer],\n",
        "#    max_episodes = 10)\n",
        "\n",
        "#timestep = train_py_env.reset()\n",
        "#driver.run(timestep)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dSr_21QPu_w",
        "outputId": "810bb93e-03c4-489a-d6a5-afeee3b25c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=(Trajectory(\n",
            "{'action': TensorSpec(shape=(30, 2), dtype=tf.int32, name=None),\n",
            " 'discount': TensorSpec(shape=(30, 2), dtype=tf.float32, name=None),\n",
            " 'next_step_type': TensorSpec(shape=(30, 2), dtype=tf.int32, name=None),\n",
            " 'observation': {'observations': TensorSpec(shape=(30, 2, 25), dtype=tf.int32, name=None),\n",
            "                 'valid_actions': TensorSpec(shape=(30, 2, 19), dtype=tf.bool, name=None)},\n",
            " 'policy_info': (),\n",
            " 'reward': TensorSpec(shape=(30, 2), dtype=tf.float32, name=None),\n",
            " 'step_type': TensorSpec(shape=(30, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(30, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(30, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(30, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(30, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(30, 2), dtype=tf.int32, name=None)))>\n"
          ]
        }
      ],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    #num_parallel_calls=3,\n",
        "    num_steps=2,\n",
        "    sample_batch_size=batch_size)#.prefetch(3)\n",
        "\n",
        "print(dataset)\n",
        "#print(dataset.cardinality)\n",
        "#print(\"he \", len(list(dataset)))\n",
        "#print(dataset.cardinality().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZoM32BiPw4V",
        "outputId": "84cbe1b2-e1fd-4144-9a96-50cbcc91b0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f28491d8ed0>\n"
          ]
        }
      ],
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMUAu1uadRHO",
        "outputId": "5fb0544f-dc82-42c9-d669-293bdfc2856f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Trajectory(\n",
              " {'action': <tf.Tensor: shape=(30, 2), dtype=int32, numpy=\n",
              " array([[18, 18],\n",
              "        [ 6, 15],\n",
              "        [18, 18],\n",
              "        [ 0, 12],\n",
              "        [ 1, 13],\n",
              "        [ 1, 11],\n",
              "        [ 5, 12],\n",
              "        [13, 12],\n",
              "        [11,  1],\n",
              "        [15,  1],\n",
              "        [ 5, 12],\n",
              "        [ 9,  1],\n",
              "        [16, 12],\n",
              "        [ 0, 12],\n",
              "        [ 6, 15],\n",
              "        [ 7, 14],\n",
              "        [16, 17],\n",
              "        [13, 12],\n",
              "        [ 1, 15],\n",
              "        [16, 15],\n",
              "        [12, 18],\n",
              "        [10,  7],\n",
              "        [14, 18],\n",
              "        [ 3,  5],\n",
              "        [ 7, 14],\n",
              "        [ 7, 18],\n",
              "        [15,  5],\n",
              "        [13, 12],\n",
              "        [14,  2],\n",
              "        [12, 14]], dtype=int32)>,\n",
              "  'discount': <tf.Tensor: shape=(30, 2), dtype=float32, numpy=\n",
              " array([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]], dtype=float32)>,\n",
              "  'next_step_type': <tf.Tensor: shape=(30, 2), dtype=int32, numpy=\n",
              " array([[1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1]], dtype=int32)>,\n",
              "  'observation': {'observations': <tf.Tensor: shape=(30, 2, 25), dtype=int32, numpy=\n",
              " array([[[ 1, 15, 17, ...,  0,  0,  0],\n",
              "         [ 1, 15, 17, ...,  0,  0,  0]],\n",
              " \n",
              "        [[ 1, 15, 17, ...,  0,  0,  0],\n",
              "         [ 1, 15, 17, ...,  0,  0,  0]],\n",
              " \n",
              "        [[ 1, 15, 17, ...,  0,  0,  0],\n",
              "         [ 1, 15, 17, ...,  0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 1, 15, 17, ...,  0,  0,  0],\n",
              "         [ 1, 15, 17, ...,  0,  0,  0]],\n",
              " \n",
              "        [[ 1, 15, 17, ...,  0,  0,  0],\n",
              "         [ 1, 15, 17, ...,  0,  0,  0]],\n",
              " \n",
              "        [[ 1, 15, 17, ...,  0,  0,  0],\n",
              "         [ 1, 15, 17, ...,  0,  0,  0]]], dtype=int32)>,\n",
              "                  'valid_actions': <tf.Tensor: shape=(30, 2, 19), dtype=bool, numpy=\n",
              " array([[[False,  True, False, ..., False, False, False],\n",
              "         [False,  True, False, ..., False, False, False]],\n",
              " \n",
              "        [[False,  True, False, ..., False, False, False],\n",
              "         [False,  True, False, ..., False, False, False]],\n",
              " \n",
              "        [[False,  True, False, ..., False, False, False],\n",
              "         [False,  True, False, ..., False, False, False]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[False,  True, False, ..., False, False, False],\n",
              "         [False,  True, False, ..., False, False, False]],\n",
              " \n",
              "        [[False, False, False, ..., False, False, False],\n",
              "         [False, False, False, ..., False, False, False]],\n",
              " \n",
              "        [[False, False, False, ..., False, False, False],\n",
              "         [False, False, False, ..., False, False, False]]])>},\n",
              "  'policy_info': (),\n",
              "  'reward': <tf.Tensor: shape=(30, 2), dtype=float32, numpy=\n",
              " array([[641.0092 , 601.0092 ],\n",
              "        [760.0095 , 720.0095 ],\n",
              "        [641.0092 , 601.0092 ],\n",
              "        [138.54654, 202.00896],\n",
              "        [519.9162 , 479.91623],\n",
              "        [759.9162 , 719.9162 ],\n",
              "        [640.68854, 600.68854],\n",
              "        [680.0095 , 640.0095 ],\n",
              "        [600.0095 , 759.9162 ],\n",
              "        [440.68854, 400.68854],\n",
              "        [640.68854, 600.68854],\n",
              "        [200.68854, 160.68854],\n",
              "        [694.0923 , 654.0923 ],\n",
              "        [138.54654, 202.00896],\n",
              "        [760.0095 , 720.0095 ],\n",
              "        [506.75446, 604.6533 ],\n",
              "        [761.0092 , 721.0092 ],\n",
              "        [680.0095 , 640.0095 ],\n",
              "        [280.68854, 240.68854],\n",
              "        [480.68854, 440.68854],\n",
              "        [202.00896, 270.46658],\n",
              "        [639.9162 , 599.9162 ],\n",
              "        [681.0092 , 641.0092 ],\n",
              "        [360.68854, 320.68854],\n",
              "        [506.75446, 604.6533 ],\n",
              "        [599.9162 , 559.9162 ],\n",
              "        [399.91623, 359.91623],\n",
              "        [680.0095 , 640.0095 ],\n",
              "        [560.68854, 520.68854],\n",
              "        [600.68854, 560.68854]], dtype=float32)>,\n",
              "  'step_type': <tf.Tensor: shape=(30, 2), dtype=int32, numpy=\n",
              " array([[1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1]], dtype=int32)>}),\n",
              " SampleInfo(key=<tf.Tensor: shape=(30, 2), dtype=uint64, numpy=\n",
              " array([[17841648262877670355, 17841648262877670355],\n",
              "        [ 7575846068285600332,  7575846068285600332],\n",
              "        [17841648262877670355, 17841648262877670355],\n",
              "        [14396044213928175887, 14396044213928175887],\n",
              "        [ 7948365150649607168,  7948365150649607168],\n",
              "        [17265914858414078053, 17265914858414078053],\n",
              "        [ 6686780903259256972,  6686780903259256972],\n",
              "        [ 9481576634921455354,  9481576634921455354],\n",
              "        [ 1966896876182327728,  1966896876182327728],\n",
              "        [ 8333320051443911113,  8333320051443911113],\n",
              "        [ 6686780903259256972,  6686780903259256972],\n",
              "        [ 5478022011446296952,  5478022011446296952],\n",
              "        [  766120224824916446,   766120224824916446],\n",
              "        [14396044213928175887, 14396044213928175887],\n",
              "        [ 7575846068285600332,  7575846068285600332],\n",
              "        [15383709669937660743, 15383709669937660743],\n",
              "        [ 1326222526973295628,  1326222526973295628],\n",
              "        [ 9481576634921455354,  9481576634921455354],\n",
              "        [15961804340826026966, 15961804340826026966],\n",
              "        [ 7264158848467891301,  7264158848467891301],\n",
              "        [12269462570460912824, 12269462570460912824],\n",
              "        [ 1332292026504441283,  1332292026504441283],\n",
              "        [17888545328983630039, 17888545328983630039],\n",
              "        [ 4175371501749336727,  4175371501749336727],\n",
              "        [15383709669937660743, 15383709669937660743],\n",
              "        [ 3150724567542133054,  3150724567542133054],\n",
              "        [13621174653219253383, 13621174653219253383],\n",
              "        [ 9481576634921455354,  9481576634921455354],\n",
              "        [10561266094997971925, 10561266094997971925],\n",
              "        [11774131430355983864, 11774131430355983864]], dtype=uint64)>, probability=<tf.Tensor: shape=(30, 2), dtype=float64, numpy=\n",
              " array([[0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138],\n",
              "        [0.01724138, 0.01724138]])>, table_size=<tf.Tensor: shape=(30, 2), dtype=int64, numpy=\n",
              " array([[58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58],\n",
              "        [58, 58]])>, priority=<tf.Tensor: shape=(30, 2), dtype=float64, numpy=\n",
              " array([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])>, times_sampled=<tf.Tensor: shape=(30, 2), dtype=int32, numpy=\n",
              " array([[1, 1],\n",
              "        [1, 1],\n",
              "        [2, 2],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [2, 2],\n",
              "        [2, 2],\n",
              "        [3, 3],\n",
              "        [2, 2],\n",
              "        [3, 3],\n",
              "        [2, 2],\n",
              "        [1, 1],\n",
              "        [3, 3],\n",
              "        [2, 2],\n",
              "        [3, 3],\n",
              "        [1, 1],\n",
              "        [3, 3],\n",
              "        [1, 1],\n",
              "        [2, 2],\n",
              "        [1, 1],\n",
              "        [5, 5],\n",
              "        [4, 4],\n",
              "        [4, 4],\n",
              "        [4, 4],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [4, 4],\n",
              "        [5, 5],\n",
              "        [5, 5]], dtype=int32)>))"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data\n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "iterator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79OJzEBi3_0n"
      },
      "outputs": [],
      "source": [
        "ds = dataset #.range(2)\n",
        "for element in ds:\n",
        "  print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs67Yo3aP1J5"
      },
      "source": [
        "# **Training the agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeZ83s1KP9nE",
        "outputId": "d38e4d20-1228-44d8-d09c-de093769a36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  3\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  4\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  5\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  6\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  7\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  8\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "********Episode*************  9\n",
            "reset\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "time_step TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "start while loop\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[1842.1091, 1842.0033, 1842.101 , 1842.1194, 1842.1041, 1842.0369,\n",
            "        1842.1105, 1842.1132, 1842.116 , 1842.1023, 1842.105 , 1842.1012,\n",
            "        1842.1025, 1842.0996, 1842.1018, 1842.1139, 1842.116 , 1842.0973,\n",
            "        1842.1031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "action_step0  3\n",
            "step:  5\n",
            "action:  5\n",
            "max_capacity  25\n",
            "Loaded_weight  0\n",
            "1Loaded_weight+  6\n",
            "gr:  12.696156175056842\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.6961565], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12.6961565], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[3035.9944, 3036.1448, 3027.4353, 3042.383 , 3009.0098, 3046.4453,\n",
            "        3027.417 , 3034.4739, 3031.9373, 3027.6416, 3016.1492, 3035.281 ,\n",
            "        3031.712 , 3020.8096, 3035.7922, 3039.198 , 3035.3909, 3033.0137,\n",
            "        3029.7637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())\n",
            "action_step0  5\n",
            "step:  7\n",
            "action:  7\n",
            "max_capacity  25\n",
            "Loaded_weight  6\n",
            "1Loaded_weight+  10\n",
            "gr:  32.97493443614345\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([32.974934], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([45.67109], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[1, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[4256.0986, 4215.3115, 4277.879 , 4231.1533, 4260.744 , 4220.961 ,\n",
            "        4257.276 , 4240.1616, 4251.2114, 4248.2363, 4263.5913, 4253.5474,\n",
            "        4246.0776, 4258.049 , 4223.4604, 4223.9927, 4222.5923, 4242.3105,\n",
            "        4288.241 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=())\n",
            "action_step0  18\n",
            "step:  20\n",
            "action:  20\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  17\n",
            "gr:  58.04721816359455\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([58.04722], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([103.71831], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[ True,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6146.28  , 6007.059 , 6146.269 , 6079.998 , 6092.894 , 6077.3916,\n",
            "        6090.8687, 6045.0835, 6081.254 , 6100.1924, 6123.9727, 6144.3564,\n",
            "        6084.759 , 6106.618 , 6055.7666, 6035.534 , 6001.73  , 6118.9473,\n",
            "        6206.45  ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "action_step0  0\n",
            "step:  2\n",
            "action:  2\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "1Loaded_weight+  19\n",
            "gr:  100.43318696844298\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.43319], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([204.15149], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True,  True, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[6644.692 , 6486.3867, 6663.431 , 6562.7036, 6600.3506, 6555.9927,\n",
            "        6604.3984, 6544.702 , 6588.948 , 6595.3315, 6635.176 , 6638.2583,\n",
            "        6576.153 , 6621.764 , 6538.0986, 6516.9316, 6485.8086, 6612.801 ,\n",
            "        6720.7256]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "action_step0  2\n",
            "step:  4\n",
            "action:  4\n",
            "max_capacity  25\n",
            "Loaded_weight  19\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  150.50460408293347\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([150.50461], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([354.6561], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[7834.256 , 7644.0083, 7889.386 , 7727.969 , 7837.002 , 7716.842 ,\n",
            "        7816.9937, 7725.9497, 7787.1016, 7785.7256, 7858.546 , 7827.3438,\n",
            "        7761.8765, 7843.9727, 7702.653 , 7680.5527, 7648.0454, 7793.8687,\n",
            "        7945.7036]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=())\n",
            "action_step0  10\n",
            "step:  12\n",
            "action:  12\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  203.60315530506847\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([203.60315], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([558.2593], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[10173.901,  9899.075, 10346.504,  9972.111, 10376.451,  9964.101,\n",
            "        10219.715, 10028.954, 10146.669, 10141.178, 10299.332, 10155.318,\n",
            "        10097.782, 10296.788,  9982.536,  9960.371,  9908.856, 10072.001,\n",
            "        10421.217]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())\n",
            "action_step0  4\n",
            "step:  6\n",
            "action:  6\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  12\n",
            "gr:  273.8256043741988\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([273.8256], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([832.08484], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[11283.926, 10964.089, 11529.719, 11034.16 , 11606.836, 11017.731,\n",
            "        11367.299, 11127.105, 11274.041, 11255.758, 11466.145, 11268.713,\n",
            "        11197.749, 11477.578, 11052.817, 11035.745, 10974.561, 11158.267,\n",
            "        11604.507]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=())\n",
            "action_step0  13\n",
            "step:  15\n",
            "action:  15\n",
            "max_capacity  25\n",
            "Loaded_weight  12\n",
            "1Loaded_weight+  14\n",
            "gr:  345.6661923897093\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([345.6662], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1177.751], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False,  True,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[14143.214 , 13683.243 , 14589.078 , 13749.338 , 14762.401 ,\n",
            "        13710.968 , 14351.837 , 13963.184 , 14195.7295, 14113.746 ,\n",
            "        14523.114 , 14123.183 , 14031.407 , 14539.975 , 13785.391 ,\n",
            "        13776.724 , 13713.636 , 13984.497 , 14649.807 ]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())\n",
            "action_step0  6\n",
            "step:  8\n",
            "action:  8\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  16\n",
            "gr:  422.60928638359746\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([422.60928], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([1600.3602], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True,  True,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[15653.023, 15159.274, 16152.805, 15217.369, 16358.672, 15177.464,\n",
            "        15887.039, 15467.914, 15714.768, 15635.986, 16091.194, 15622.481,\n",
            "        15532.228, 16094.871, 15263.054, 15259.509, 15197.826, 15466.036,\n",
            "        16215.265]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=())\n",
            "action_step0  8\n",
            "step:  10\n",
            "action:  10\n",
            "max_capacity  25\n",
            "Loaded_weight  16\n",
            "1Loaded_weight+  17\n",
            "gr:  505.8545750963326\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([505.85458], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2106.2148], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[17533.453, 16968.557, 18148.195, 17026.234, 18403.27 , 16979.832,\n",
            "        17837.17 , 17339.582, 17627.701, 17558.143, 18098.25 , 17491.58 ,\n",
            "        17441.281, 18076.78 , 17089.133, 17083.125, 17026.135, 17303.38 ,\n",
            "        18201.605]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=())\n",
            "action_step0  9\n",
            "step:  11\n",
            "action:  11\n",
            "max_capacity  25\n",
            "Loaded_weight  17\n",
            "add an other car\n",
            "Loaded_weight+  8\n",
            "gr:  595.0877966360515\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([595.08777], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([2701.3027], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False,  True,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19266.803, 18614.418, 19997.98 , 18674.14 , 20317.271, 18629.87 ,\n",
            "        19627.836, 19034.535, 19380.406, 19337.533, 19968.709, 19210.264,\n",
            "        19203.316, 19906.246, 18785.635, 18750.973, 18682.31 , 18975.93 ,\n",
            "        20060.303]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())\n",
            "action_step0  11\n",
            "step:  13\n",
            "action:  13\n",
            "max_capacity  25\n",
            "Loaded_weight  8\n",
            "1Loaded_weight+  9\n",
            "gr:  701.6244972804978\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([701.6245], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([3402.9272], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False,  True, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[19950.338, 19220.53 , 20783.467, 19271.223, 21169.094, 19229.492,\n",
            "        20352.09 , 19681.182, 20067.404, 20048.914, 20756.521, 19884.652,\n",
            "        19899.084, 20680.486, 19423.21 , 19369.387, 19290.648, 19606.914,\n",
            "        20858.443]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=())\n",
            "action_step0  12\n",
            "step:  14\n",
            "action:  14\n",
            "max_capacity  25\n",
            "Loaded_weight  9\n",
            "1Loaded_weight+  10\n",
            "gr:  822.921985697443\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([822.922], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([4225.849], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21414.744, 20634.025, 22379.033, 20622.818, 22869.182, 20585.69 ,\n",
            "        21909.396, 21125.238, 21562.264, 21588.25 , 22384.727, 21311.887,\n",
            "        21414.662, 22270.824, 20876.041, 20806.432, 20698.596, 20973.484,\n",
            "        22453.637]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=())\n",
            "action_step0  7\n",
            "step:  9\n",
            "action:  9\n",
            "max_capacity  25\n",
            "Loaded_weight  10\n",
            "1Loaded_weight+  18\n",
            "gr:  959.8705458312716\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([959.87054], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([5185.7197], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True,  True,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21831.191, 21032.78 , 22805.611, 21016.203, 23333.14 , 20998.246,\n",
            "        22324.293, 21511.287, 21959.594, 22017.16 , 22832.543, 21723.176,\n",
            "        21841.78 , 22681.188, 21287.04 , 21214.865, 21087.967, 21365.533,\n",
            "        22897.031]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=())\n",
            "action_step0  17\n",
            "step:  19\n",
            "action:  19\n",
            "max_capacity  25\n",
            "Loaded_weight  18\n",
            "1Loaded_weight+  22\n",
            "gr:  1106.461139976541\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1106.4612], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([6292.1807], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[21205.68 , 20412.2  , 22120.13 , 20400.906, 22710.328, 20413.973,\n",
            "        21616.254, 20797.691, 21248.404, 21408.105, 22192.3  , 21120.043,\n",
            "        21241.994, 21978.31 , 20695.043, 20604.879, 20413.688, 20704.582,\n",
            "        22271.053]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=())\n",
            "action_step0  14\n",
            "step:  16\n",
            "action:  16\n",
            "max_capacity  25\n",
            "Loaded_weight  22\n",
            "add an other car\n",
            "Loaded_weight+  4\n",
            "gr:  1266.42181427709\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1266.4219], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([7558.6025], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16,  0,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False,  True,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[23306.184, 22449.709, 24319.338, 22345.783, 24977.238, 22449.812,\n",
            "        23767.191, 22844.367, 23344.871, 23562.893, 24420.285, 23116.904,\n",
            "        23375.28 , 24147.836, 22778.543, 22666.469, 22433.053, 22618.967,\n",
            "        24497.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([15], dtype=int32)>, state=(), info=())\n",
            "action_step0  15\n",
            "step:  17\n",
            "action:  17\n",
            "max_capacity  25\n",
            "Loaded_weight  4\n",
            "1Loaded_weight+  7\n",
            "gr:  1436.5101433175162\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1436.5101], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([8995.112], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  0,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22351.656, 21543.064, 23270.238, 21455.398, 23922.57 , 21567.209,\n",
            "        22754.127, 21860.865, 22365.299, 22611.477, 23396.438, 22159.008,\n",
            "        22444.043, 23114.795, 21886.355, 21755.635, 21503.973, 21646.916,\n",
            "        23480.312]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "action_step0  1\n",
            "step:  3\n",
            "action:  3\n",
            "max_capacity  25\n",
            "Loaded_weight  7\n",
            "1Loaded_weight+  14\n",
            "gr:  1617.019445785663\n",
            "ending episode1 False\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1617.0194], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([10612.132], shape=(1,), dtype=float32)\n",
            "@@@@@@@@@@@@@in loop@@@@@@@@@@@@@\n",
            "obs:  {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3,  0,  0,  0]], dtype=int32)>, 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False,\n",
            "        False]])>}\n",
            "qnet_obs (<tf.Tensor: shape=(1, 19), dtype=float32, numpy=\n",
            "array([[22095.303, 21288.467, 23009.402, 21200.607, 23666.482, 21316.771,\n",
            "        22488.842, 21599.785, 22102.553, 22358.68 , 23137.857, 21902.518,\n",
            "        22191.684, 22850.256, 21637.348, 21503.213, 21245.205, 21376.406,\n",
            "        23225.783]], dtype=float32)>, ())\n",
            "nact:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(18, dtype=int32))\n",
            "action_step:  PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())\n",
            "action_step0  16\n",
            "step:  18\n",
            "action:  18\n",
            "max_capacity  25\n",
            "Loaded_weight  14\n",
            "1Loaded_weight+  18\n",
            "gr:  1817.52874825381\n",
            "ending episode1 True\n",
            "time_step  TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'observation': {'observations': <tf.Tensor: shape=(1, 25), dtype=int32, numpy=\n",
            "array([[ 1,  5,  7, 20,  2,  1,  4, 12,  6, 15,  8, 10,  1, 11, 13, 14,\n",
            "         9, 19,  1, 16, 17,  3, 18,  1,  0]], dtype=int32)>,\n",
            "                 'valid_actions': <tf.Tensor: shape=(1, 19), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False]])>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1817.5288], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
            "distribution  PolicyStep(action=<tfp.distributions.Deterministic 'Deterministic' batch_shape=[1] event_shape=[] dtype=int32>, state=(), info=())\n",
            "episode_return  tf.Tensor([12429.66], shape=(1,), dtype=float32)\n",
            "step = 3000: Average Return = 12429.6591796875\n"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "print(\"a\")\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "print(\"b\")\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "print(\"c\")\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "print(\"d \", time_step)\n",
        "env.reset()\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_episodes = 1)#collect_steps_per_iteration\n",
        "print(\"e\")\n",
        "#sys.exit(-1)\n",
        "for _ in range(num_iterations):\n",
        "  if time_step.is_last():\n",
        "    time_step = train_py_env.reset()\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "  print(\"ts \", time_step)\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  #print(\"experience: \", experience)\n",
        "  training = agent.train(experience)\n",
        "  print(training)\n",
        "  train_loss = training.loss #agent.train(experience).loss\n",
        "  print(\"tl \", train_loss)\n",
        "  step = agent.train_step_counter.numpy()\n",
        "  #sys.exit(-1)\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(iterations)\n",
        "print(returns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt5z_XaSZqK8",
        "outputId": "0ae4944e-a1e1-407f-cfde-29a0446af1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "range(0, 10001, 300)\n",
            "[12237.311, 13456.404, 12155.943, 11724.524, 14020.055, 11538.713, 12883.175, 11545.782, 11412.265, 13536.545, 12430.409, 14041.221, 10756.635, 13316.143, 11222.393, 11020.148, 11647.1, 14011.943, 14266.422, 15867.521, 14262.939, 15167.689, 10558.103, 11445.896, 13997.041, 14695.514, 9428.807, 11558.757, 10649.342, 12539.842, 12429.659]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkTdBdFxQAv_"
      },
      "source": [
        "# **Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBLhDBN1QB0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4e96f2ad-72f7-483f-ba60-e921bf1afca3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns, color='red')\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "#plt.ylim(top=250)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}